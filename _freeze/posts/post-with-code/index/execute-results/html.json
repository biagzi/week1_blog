{
  "hash": "b7abcbcaf35bc712f6686c2687632452",
  "result": {
    "markdown": "---\ntitle: \"Bayesian regression\"\nauthor: \"Renata B. Biazzi\"\ndate: \"2024-04-27\"\ncategories: [news, code, analysis]\nimage: \"Petal-sepal.jpg\"\n---\n\n\nIn this tutorial, I will show how to do a **Bayesian regression** using [brms](https://cran.r-project.org/web/packages/brms/index.html)!\n\nThe first part is loading the necessary packages:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(datasets) #to open the dataset\nlibrary(rstan) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: StanHeaders\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nrstan version 2.32.6 (Stan version 2.32.2)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\nFor within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\nchange `threads_per_chain` option:\nrstan_options(threads_per_chain = 1)\n```\n:::\n\n```{.r .cell-code}\nlibrary(brms)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: Rcpp\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading 'brms' package (version 2.21.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'brms'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:rstan':\n\n    loo\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:stats':\n\n    ar\n```\n:::\n\n```{.r .cell-code}\nlibrary(psych) #to summary statistics\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'psych'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:brms':\n\n    cs\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:rstan':\n\n    lookup\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyverse) # for data manipulation and plotting\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::%+%()   masks psych::%+%()\n✖ ggplot2::alpha() masks psych::alpha()\n✖ tidyr::extract() masks rstan::extract()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n:::\n\n\nIn this tutorial, we will use data from the Iris data set - maybe one of the most famous data sets ever. It was used by the British statistician and biologist Ronald Fisher to show how to distinguish three different, but related, species of Iris flowers using their morphological features. Edgar Anderson collected the data when he was trying to quantify the morphological variation of them.\n\nData description:\n\n50 samples from each of the three species ([*Iris setosa*](https://en.wikipedia.org/wiki/Iris_setosa \"Iris setosa\"), [*Iris virginica*](https://en.wikipedia.org/wiki/Iris_virginica \"Iris virginica\") and [*Iris versicolor*](https://en.wikipedia.org/wiki/Iris_versicolor \"Iris versicolor\"));\n\nFeatures:\n\n-   length of the sepals\n\n-   length of the petals\n\n-   width of the sepals\n\n-   width of the petals.\n\nOpening and inspecting the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(iris)\nsummary(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n```\n:::\n:::\n\n\nExploring the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(iris, aes(x = Sepal.Length)) +\n  geom_histogram(binwidth = 0.1, fill = \"pink\", color = \"black\") +\n  labs(title = \"Sepal.Length Distribution\", x = \"Sepal.Length\", y = \"Frequency\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(iris, aes(x = Sepal.Width)) +\n  geom_histogram(binwidth = 0.1, fill = \"pink\", color = \"black\") +\n  labs(title = \"Sepal.Width Distribution\", x = \"Sepal.Width\", y = \"Frequency\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(iris, aes(x = Petal.Length)) +\n  geom_histogram(binwidth = 0.1, fill = \"pink\", color = \"black\") +\n  labs(title = \"Petal.Length Distribution\", x = \"Petal.Length\", y = \"Frequency\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-3.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(iris, aes(x = Petal.Width)) +\n  geom_histogram(binwidth = 0.1, fill = \"pink\", color = \"black\") +\n  labs(title = \"Petal.Width Distribution\", x = \"Petal.Width\", y = \"Frequency\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-4.png){width=672}\n:::\n:::\n\n\nIn this tutorial we will investigate the impact of sepal and petals length on the sepal width (our outcome variable) independent of the species. In the future, you can try to do other interesting analysis, as assessing how much this features predict the length of the petals for each one of the species.\n\nIn a Bayesian approach, we use a first belief of how likely is some hypothesis (prior) to be able to derive the probability of this hypothesis (posterior) after using the prior with the data we have.\n\nThe specification of the prior distribution is a crucial point. In our case, we will use a normal distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Specifying our prior distribution\nprior <- brms::prior(normal(0, 10), class=b) #set normal prior on regression coefficients (mean of 0, location of 3)\n```\n:::\n\n\nSo, now we run or Bayesian regression by specifying our model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- brm(formula = Sepal.Width ~ Sepal.Length + Petal.Length, \n             data    = iris,\n             seed    = 123,\n             prior = prior,\n             chains=4, # how many chains are run\n            iter = 500) # number of MCMC samples\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nCompiling Stan program...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nTrying to compile a simple C file\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 15.0.0 (clang-1500.3.9.4)’\nusing SDK: ‘MacOSX14.4.sdk’\nclang -arch arm64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from <built-in>:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n#include <cmath>\n         ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nStart sampling\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:   1 / 500 [  0%]  (Warmup)\nChain 1: Iteration:  50 / 500 [ 10%]  (Warmup)\nChain 1: Iteration: 100 / 500 [ 20%]  (Warmup)\nChain 1: Iteration: 150 / 500 [ 30%]  (Warmup)\nChain 1: Iteration: 200 / 500 [ 40%]  (Warmup)\nChain 1: Iteration: 250 / 500 [ 50%]  (Warmup)\nChain 1: Iteration: 251 / 500 [ 50%]  (Sampling)\nChain 1: Iteration: 300 / 500 [ 60%]  (Sampling)\nChain 1: Iteration: 350 / 500 [ 70%]  (Sampling)\nChain 1: Iteration: 400 / 500 [ 80%]  (Sampling)\nChain 1: Iteration: 450 / 500 [ 90%]  (Sampling)\nChain 1: Iteration: 500 / 500 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.004 seconds (Warm-up)\nChain 1:                0.004 seconds (Sampling)\nChain 1:                0.008 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:   1 / 500 [  0%]  (Warmup)\nChain 2: Iteration:  50 / 500 [ 10%]  (Warmup)\nChain 2: Iteration: 100 / 500 [ 20%]  (Warmup)\nChain 2: Iteration: 150 / 500 [ 30%]  (Warmup)\nChain 2: Iteration: 200 / 500 [ 40%]  (Warmup)\nChain 2: Iteration: 250 / 500 [ 50%]  (Warmup)\nChain 2: Iteration: 251 / 500 [ 50%]  (Sampling)\nChain 2: Iteration: 300 / 500 [ 60%]  (Sampling)\nChain 2: Iteration: 350 / 500 [ 70%]  (Sampling)\nChain 2: Iteration: 400 / 500 [ 80%]  (Sampling)\nChain 2: Iteration: 450 / 500 [ 90%]  (Sampling)\nChain 2: Iteration: 500 / 500 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.004 seconds (Warm-up)\nChain 2:                0.004 seconds (Sampling)\nChain 2:                0.008 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:   1 / 500 [  0%]  (Warmup)\nChain 3: Iteration:  50 / 500 [ 10%]  (Warmup)\nChain 3: Iteration: 100 / 500 [ 20%]  (Warmup)\nChain 3: Iteration: 150 / 500 [ 30%]  (Warmup)\nChain 3: Iteration: 200 / 500 [ 40%]  (Warmup)\nChain 3: Iteration: 250 / 500 [ 50%]  (Warmup)\nChain 3: Iteration: 251 / 500 [ 50%]  (Sampling)\nChain 3: Iteration: 300 / 500 [ 60%]  (Sampling)\nChain 3: Iteration: 350 / 500 [ 70%]  (Sampling)\nChain 3: Iteration: 400 / 500 [ 80%]  (Sampling)\nChain 3: Iteration: 450 / 500 [ 90%]  (Sampling)\nChain 3: Iteration: 500 / 500 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.007 seconds (Warm-up)\nChain 3:                0.005 seconds (Sampling)\nChain 3:                0.012 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:   1 / 500 [  0%]  (Warmup)\nChain 4: Iteration:  50 / 500 [ 10%]  (Warmup)\nChain 4: Iteration: 100 / 500 [ 20%]  (Warmup)\nChain 4: Iteration: 150 / 500 [ 30%]  (Warmup)\nChain 4: Iteration: 200 / 500 [ 40%]  (Warmup)\nChain 4: Iteration: 250 / 500 [ 50%]  (Warmup)\nChain 4: Iteration: 251 / 500 [ 50%]  (Sampling)\nChain 4: Iteration: 300 / 500 [ 60%]  (Sampling)\nChain 4: Iteration: 350 / 500 [ 70%]  (Sampling)\nChain 4: Iteration: 400 / 500 [ 80%]  (Sampling)\nChain 4: Iteration: 450 / 500 [ 90%]  (Sampling)\nChain 4: Iteration: 500 / 500 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.006 seconds (Warm-up)\nChain 4:                0.004 seconds (Sampling)\nChain 4:                0.01 seconds (Total)\nChain 4: \n```\n:::\n:::\n\n\nVisualize the distribution of values for each parameters and the chains convergence.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nNow we will analyze and interpret the coefficients:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Sepal.Width ~ Sepal.Length + Petal.Length \n   Data: iris (Number of observations: 150) \n  Draws: 4 chains, each with iter = 500; warmup = 250; thin = 1;\n         total post-warmup draws = 1000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept        1.04      0.29     0.47     1.62 1.01      601      667\nSepal.Length     0.56      0.07     0.43     0.69 1.01      514      461\nPetal.Length    -0.33      0.03    -0.40    -0.28 1.01      529      437\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.33      0.02     0.29     0.36 1.00      626      620\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\nWe can see from the summary that our chains have converged sufficiently (rhat close to 1).\n\nThe length of the sepals seems to be a relevant predictor of the length of the petals for iris flowers, with a posterior mean regression coefficient of 1.78, 95% Credibility Interval \\[1.65, 1.90\\]. The length of the petals are also relevant predictors, with a posterior mean of -0.33, and a 95% credibility Interval of \\[-1.40, -0.28\\]. Since the intervals do not include zero, we can be fairly sure there is an effect.\n\nNow, we will analyze the posterior distribution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(model, ndraws=100)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nAnalyzing the posterior, we see that the samples from the posterior predictive distribution (y_rep) seems to capture really well the observed data (y).\n\nNext week we will see how to make our model better!\n\nWe can try other priors and then compare different models. We can also test interaction terms between the predictors, and check if all the predictor variables are really necessary.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}