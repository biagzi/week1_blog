{
  "hash": "53d33effbf6051c3ccb6e6e25f0ea564",
  "result": {
    "markdown": "---\ntitle: \"Bayesian regression\"\nauthor: \"Renata B. Biazzi\"\ndate: \"2024-04-27\"\ncategories: [news, code, analysis]\nimage: \"Petal-sepal.jpg\"\n---\n\n\nIn this tutorial, I will show how to do a **Bayesian regression** using [brms](https://cran.r-project.org/web/packages/brms/index.html)!\n\nSo, the first thing to answer is why use a Bayesian regression..\n\nCompared to an Ordinary Least Squares regression, a Bayesian regression allows us to determine distributions for the model parameters, instead of point estimate and a confidence interval. It is based on the **Bayes' Rule,** a theorem that defines how the events of conditional probabilities relate with each other:\n\n$$\nP[A|B] = \\frac{P[A \\& B]}{P[B]} = P[B|A] \\frac{P[A]}{P[B]}\n$$\n\nIn our case, we want to estimate A (the model parameters) given B (some data):\n\n$$\nP[model|data] = P[data|model] \\frac{P[model]}{P[data]}\n$$\n\nIn Bayesian inference (we want to infer the model parameters) we use some especif names for each term of the equation:\n\n$$\nPosterior = Likelihood \\frac{Prior}{Normalization}\n$$\n\nThe Posterior is the posterior probability distribution of the model parameters that we want to estimate. The Likelihood is the probability of observing the data given the model we are considering. The Prior is the prior probability distribution for the model parameters. If we have knowledge of the phenomenon, we can include it in the model by choosing some specific prior that reflects our knowledge. However, if we don't have any previous expectations, we can use non-informative priors (such as a normal distribution) for the parameters.\n\nNow we can follow with the application of these ideas:\n\nThe first part is loading the necessary packages:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(datasets) #to open the dataset\nlibrary(rstan) \nlibrary(brms)\nlibrary(knitr)\nlibrary(psych) #to summary statistics\nlibrary(tidyverse) # for data manipulation and plotting\nlibrary(marginaleffects) # get posteriors \nlibrary(ggeffects) # graph\nlibrary(easystats) # easystats packages # bayesttestR \nlibrary(bayesplot) # graph trace plots\nlibrary(ggdist) # graph distributions and geoms\n```\n:::\n\n\nIn this tutorial, we will use data from the Iris data set - maybe one of the most famous data sets ever. It was used by the British statistician and biologist Ronald Fisher to show how to distinguish three different, but related, species of Iris flowers using their morphological features. Edgar Anderson collected the data when he was trying to quantify the morphological variation of them.\n\nData description:\n\n50 samples from each of the three species ([*Iris setosa*](https://en.wikipedia.org/wiki/Iris_setosa \"Iris setosa\"), [*Iris virginica*](https://en.wikipedia.org/wiki/Iris_virginica \"Iris virginica\") and [*Iris versicolor*](https://en.wikipedia.org/wiki/Iris_versicolor \"Iris versicolor\"));\n\nFeatures:\n\n-   length of the sepals\n\n-   length of the petals\n\n-   width of the sepals\n\n-   width of the petals.\n\nOpening and inspecting the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(iris)\nsummary(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n```\n:::\n:::\n\n\nExploring the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(iris, aes(x = Sepal.Length)) +\n  geom_histogram(binwidth = 0.1, fill = \"pink\", color = \"black\") +\n  labs(title = \"Sepal.Length Distribution\", x = \"Sepal.Length\", y = \"Frequency\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(iris, aes(x = Sepal.Width)) +\n  geom_histogram(binwidth = 0.1, fill = \"pink\", color = \"black\") +\n  labs(title = \"Sepal.Width Distribution\", x = \"Sepal.Width\", y = \"Frequency\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(iris, aes(x = Petal.Length)) +\n  geom_histogram(binwidth = 0.1, fill = \"pink\", color = \"black\") +\n  labs(title = \"Petal.Length Distribution\", x = \"Petal.Length\", y = \"Frequency\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-3.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(iris, aes(x = Petal.Width)) +\n  geom_histogram(binwidth = 0.1, fill = \"pink\", color = \"black\") +\n  labs(title = \"Petal.Width Distribution\", x = \"Petal.Width\", y = \"Frequency\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-4.png){width=672}\n:::\n:::\n\n\nIn this tutorial we will investigate the impact of sepal and petals length on the sepal width (our outcome variable), independent of the species. In the future, you can try to do other interesting analysis, as assessing how much these features predict the length of the petals for each one of the species.\n\nIn a Bayesian approach, we use a first belief of how likely is some hypothesis (prior) to be able to derive the probability of this hypothesis (posterior) after using the prior with the data we have.\n\nThe specification of the prior distribution is a crucial point. In our case, we will use a normal distribution (a non-informative prior). This distribution represent our prior belief on the parameters's distribution, in our case, we are considering it almost like a null hypothesis (if the parameters of our model are zero, this is equivalent to say there is no relationship between the dependent and independent variables).​\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Specifying our prior distribution\nprior <- brms::prior(normal(0, 10), class=b) #set normal prior on regression coefficients (mean of 0,std 10)\n```\n:::\n\n\nSo, now we run or Bayesian regression by specifying our model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- brm(formula = Sepal.Width ~ Sepal.Length + Petal.Length, \n             data    = iris,\n             seed    = 123,\n             prior = prior,\n             chains=4, # how many chains are run\n            iter = 500) # number of MCMC samples\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nusing C compiler: ‘Apple clang version 15.0.0 (clang-1500.3.9.4)’\nusing SDK: ‘MacOSX14.4.sdk’\nclang -arch arm64 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/opt/R/arm64/include    -fPIC  -falign-functions=64 -Wall -g -O2  -c foo.c -o foo.o\nIn file included from <built-in>:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n#include <cmath>\n         ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.8e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:   1 / 500 [  0%]  (Warmup)\nChain 1: Iteration:  50 / 500 [ 10%]  (Warmup)\nChain 1: Iteration: 100 / 500 [ 20%]  (Warmup)\nChain 1: Iteration: 150 / 500 [ 30%]  (Warmup)\nChain 1: Iteration: 200 / 500 [ 40%]  (Warmup)\nChain 1: Iteration: 250 / 500 [ 50%]  (Warmup)\nChain 1: Iteration: 251 / 500 [ 50%]  (Sampling)\nChain 1: Iteration: 300 / 500 [ 60%]  (Sampling)\nChain 1: Iteration: 350 / 500 [ 70%]  (Sampling)\nChain 1: Iteration: 400 / 500 [ 80%]  (Sampling)\nChain 1: Iteration: 450 / 500 [ 90%]  (Sampling)\nChain 1: Iteration: 500 / 500 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.006 seconds (Warm-up)\nChain 1:                0.005 seconds (Sampling)\nChain 1:                0.011 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:   1 / 500 [  0%]  (Warmup)\nChain 2: Iteration:  50 / 500 [ 10%]  (Warmup)\nChain 2: Iteration: 100 / 500 [ 20%]  (Warmup)\nChain 2: Iteration: 150 / 500 [ 30%]  (Warmup)\nChain 2: Iteration: 200 / 500 [ 40%]  (Warmup)\nChain 2: Iteration: 250 / 500 [ 50%]  (Warmup)\nChain 2: Iteration: 251 / 500 [ 50%]  (Sampling)\nChain 2: Iteration: 300 / 500 [ 60%]  (Sampling)\nChain 2: Iteration: 350 / 500 [ 70%]  (Sampling)\nChain 2: Iteration: 400 / 500 [ 80%]  (Sampling)\nChain 2: Iteration: 450 / 500 [ 90%]  (Sampling)\nChain 2: Iteration: 500 / 500 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.005 seconds (Warm-up)\nChain 2:                0.005 seconds (Sampling)\nChain 2:                0.01 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:   1 / 500 [  0%]  (Warmup)\nChain 3: Iteration:  50 / 500 [ 10%]  (Warmup)\nChain 3: Iteration: 100 / 500 [ 20%]  (Warmup)\nChain 3: Iteration: 150 / 500 [ 30%]  (Warmup)\nChain 3: Iteration: 200 / 500 [ 40%]  (Warmup)\nChain 3: Iteration: 250 / 500 [ 50%]  (Warmup)\nChain 3: Iteration: 251 / 500 [ 50%]  (Sampling)\nChain 3: Iteration: 300 / 500 [ 60%]  (Sampling)\nChain 3: Iteration: 350 / 500 [ 70%]  (Sampling)\nChain 3: Iteration: 400 / 500 [ 80%]  (Sampling)\nChain 3: Iteration: 450 / 500 [ 90%]  (Sampling)\nChain 3: Iteration: 500 / 500 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.008 seconds (Warm-up)\nChain 3:                0.006 seconds (Sampling)\nChain 3:                0.014 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:   1 / 500 [  0%]  (Warmup)\nChain 4: Iteration:  50 / 500 [ 10%]  (Warmup)\nChain 4: Iteration: 100 / 500 [ 20%]  (Warmup)\nChain 4: Iteration: 150 / 500 [ 30%]  (Warmup)\nChain 4: Iteration: 200 / 500 [ 40%]  (Warmup)\nChain 4: Iteration: 250 / 500 [ 50%]  (Warmup)\nChain 4: Iteration: 251 / 500 [ 50%]  (Sampling)\nChain 4: Iteration: 300 / 500 [ 60%]  (Sampling)\nChain 4: Iteration: 350 / 500 [ 70%]  (Sampling)\nChain 4: Iteration: 400 / 500 [ 80%]  (Sampling)\nChain 4: Iteration: 450 / 500 [ 90%]  (Sampling)\nChain 4: Iteration: 500 / 500 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.007 seconds (Warm-up)\nChain 4:                0.005 seconds (Sampling)\nChain 4:                0.012 seconds (Total)\nChain 4: \n```\n:::\n:::\n\n\nVisualize the distribution of values for each parameters and the chains convergence.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nNow we will analyze and interpret the coefficients:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Sepal.Width ~ Sepal.Length + Petal.Length \n   Data: iris (Number of observations: 150) \n  Draws: 4 chains, each with iter = 500; warmup = 250; thin = 1;\n         total post-warmup draws = 1000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept        1.04      0.29     0.47     1.62 1.01      601      667\nSepal.Length     0.56      0.07     0.43     0.69 1.01      514      461\nPetal.Length    -0.33      0.03    -0.40    -0.28 1.01      529      437\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.33      0.02     0.29     0.36 1.00      626      620\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n:::\n:::\n\n\nWe can see from the summary that our chains have converged sufficiently (rhat close to 1).\n\nThe length of the sepals seems to be a relevant predictor of the length of the petals for iris flowers, with a posterior mean regression coefficient of 1.78, 95% Credibility Interval \\[1.65, 1.90\\]. The length of the petals are also relevant predictors, with a posterior mean of -0.33, and a 95% credibility Interval of \\[-1.40, -0.28\\]. Since the intervals do not include zero, we can be fairly sure there is an effect.\n\nNow, we will analyze the posterior distribution:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npp_check(model, ndraws=100)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nAnalyzing the posterior, we see that the samples from the posterior predictive distribution (y_rep) seems to capture really well the observed data (y).Thus, we learned that the sepal width is not independent on the length of the sepals and the length of the petals. We also learned the distributions of values for our model parameters that can significantly predict sepal width given the length of the sepals and petals.\n\nNext week we will see how to make our model better!\n\nWe can try other priors and then compare different models. We can also test interaction terms between the predictors, and check if all the predictor variables are really necessary.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}