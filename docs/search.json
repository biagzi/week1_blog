[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is a project for PSY 504 - Advanced Statistics: Advanced Statistical Methods in Psychology.\nFeel free to contact me :)"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is my first post in a Quarto blog. Welcome to my blog!!\nI will share with you some statistic concepts that I found super interesting."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Bayesian regression",
    "section": "",
    "text": "In this tutorial, I will show how to do a Bayesian regression using brms!\nThe first part is loading the necessary packages:\n\nlibrary(datasets) #to open the dataset\nlibrary(rstan) \n\nLoading required package: StanHeaders\n\n\n\nrstan version 2.32.6 (Stan version 2.32.2)\n\n\nFor execution on a local, multicore CPU with excess RAM we recommend calling\noptions(mc.cores = parallel::detectCores()).\nTo avoid recompilation of unchanged Stan programs, we recommend calling\nrstan_options(auto_write = TRUE)\nFor within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\nchange `threads_per_chain` option:\nrstan_options(threads_per_chain = 1)\n\nlibrary(brms)\n\nLoading required package: Rcpp\n\n\nLoading 'brms' package (version 2.21.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\n\nAttaching package: 'brms'\n\n\nThe following object is masked from 'package:rstan':\n\n    loo\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\nlibrary(psych) #to summary statistics\n\n\nAttaching package: 'psych'\n\n\nThe following object is masked from 'package:brms':\n\n    cs\n\n\nThe following object is masked from 'package:rstan':\n\n    lookup\n\nlibrary(tidyverse) # for data manipulation and plotting\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::%+%()   masks psych::%+%()\n✖ ggplot2::alpha() masks psych::alpha()\n✖ tidyr::extract() masks rstan::extract()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nIn this tutorial, we will use data from the Iris data set - maybe one of the most famous data sets ever. It was used by the British statistician and biologist Ronald Fisher to show how to distinguish three different, but related, species of Iris flowers using their morphological features. Edgar Anderson collected the data when he was trying to quantify the morphological variation of them.\nData description:\n50 samples from each of the three species (Iris setosa, Iris virginica and Iris versicolor);\nFeatures:\n\nlength of the sepals\nlength of the petals\nwidth of the sepals\nwidth of the petals.\n\nOpening and inspecting the data:\n\ndata(iris)\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\nExploring the data:\n\nggplot(iris, aes(x = Sepal.Length)) +\n  geom_histogram(binwidth = 0.1, fill = \"pink\", color = \"black\") +\n  labs(title = \"Sepal.Length Distribution\", x = \"Sepal.Length\", y = \"Frequency\")\n\n\n\nggplot(iris, aes(x = Sepal.Width)) +\n  geom_histogram(binwidth = 0.1, fill = \"pink\", color = \"black\") +\n  labs(title = \"Sepal.Width Distribution\", x = \"Sepal.Width\", y = \"Frequency\")\n\n\n\nggplot(iris, aes(x = Petal.Length)) +\n  geom_histogram(binwidth = 0.1, fill = \"pink\", color = \"black\") +\n  labs(title = \"Petal.Length Distribution\", x = \"Petal.Length\", y = \"Frequency\")\n\n\n\nggplot(iris, aes(x = Petal.Width)) +\n  geom_histogram(binwidth = 0.1, fill = \"pink\", color = \"black\") +\n  labs(title = \"Petal.Width Distribution\", x = \"Petal.Width\", y = \"Frequency\")\n\n\n\n\nIn this tutorial we will investigate the impact of sepal and petals length on the sepal width (our outcome variable), independent of the species. In the future, you can try to do other interesting analysis, as assessing how much these features predict the length of the petals for each one of the species.\nIn a Bayesian approach, we use a first belief of how likely is some hypothesis (prior) to be able to derive the probability of this hypothesis (posterior) after using the prior with the data we have.\nThe specification of the prior distribution is a crucial point. In our case, we will use a normal distribution. This distribution represent our prior belief on the parameters’s distribution, in our case, we are considering it almost like a null hypothesis (if the parameters of our model are zero, this is equivalent to say there is no relationship between the dependent and independent variables).​\n\n# Specifying our prior distribution\nprior &lt;- brms::prior(normal(0, 10), class=b) #set normal prior on regression coefficients (mean of 0,std 10)\n\nSo, now we run or Bayesian regression by specifying our model:\n\nmodel &lt;- brm(formula = Sepal.Width ~ Sepal.Length + Petal.Length, \n             data    = iris,\n             seed    = 123,\n             prior = prior,\n             chains=4, # how many chains are run\n            iter = 500) # number of MCMC samples\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.6e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:   1 / 500 [  0%]  (Warmup)\nChain 1: Iteration:  50 / 500 [ 10%]  (Warmup)\nChain 1: Iteration: 100 / 500 [ 20%]  (Warmup)\nChain 1: Iteration: 150 / 500 [ 30%]  (Warmup)\nChain 1: Iteration: 200 / 500 [ 40%]  (Warmup)\nChain 1: Iteration: 250 / 500 [ 50%]  (Warmup)\nChain 1: Iteration: 251 / 500 [ 50%]  (Sampling)\nChain 1: Iteration: 300 / 500 [ 60%]  (Sampling)\nChain 1: Iteration: 350 / 500 [ 70%]  (Sampling)\nChain 1: Iteration: 400 / 500 [ 80%]  (Sampling)\nChain 1: Iteration: 450 / 500 [ 90%]  (Sampling)\nChain 1: Iteration: 500 / 500 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.004 seconds (Warm-up)\nChain 1:                0.004 seconds (Sampling)\nChain 1:                0.008 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:   1 / 500 [  0%]  (Warmup)\nChain 2: Iteration:  50 / 500 [ 10%]  (Warmup)\nChain 2: Iteration: 100 / 500 [ 20%]  (Warmup)\nChain 2: Iteration: 150 / 500 [ 30%]  (Warmup)\nChain 2: Iteration: 200 / 500 [ 40%]  (Warmup)\nChain 2: Iteration: 250 / 500 [ 50%]  (Warmup)\nChain 2: Iteration: 251 / 500 [ 50%]  (Sampling)\nChain 2: Iteration: 300 / 500 [ 60%]  (Sampling)\nChain 2: Iteration: 350 / 500 [ 70%]  (Sampling)\nChain 2: Iteration: 400 / 500 [ 80%]  (Sampling)\nChain 2: Iteration: 450 / 500 [ 90%]  (Sampling)\nChain 2: Iteration: 500 / 500 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.004 seconds (Warm-up)\nChain 2:                0.004 seconds (Sampling)\nChain 2:                0.008 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:   1 / 500 [  0%]  (Warmup)\nChain 3: Iteration:  50 / 500 [ 10%]  (Warmup)\nChain 3: Iteration: 100 / 500 [ 20%]  (Warmup)\nChain 3: Iteration: 150 / 500 [ 30%]  (Warmup)\nChain 3: Iteration: 200 / 500 [ 40%]  (Warmup)\nChain 3: Iteration: 250 / 500 [ 50%]  (Warmup)\nChain 3: Iteration: 251 / 500 [ 50%]  (Sampling)\nChain 3: Iteration: 300 / 500 [ 60%]  (Sampling)\nChain 3: Iteration: 350 / 500 [ 70%]  (Sampling)\nChain 3: Iteration: 400 / 500 [ 80%]  (Sampling)\nChain 3: Iteration: 450 / 500 [ 90%]  (Sampling)\nChain 3: Iteration: 500 / 500 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.007 seconds (Warm-up)\nChain 3:                0.004 seconds (Sampling)\nChain 3:                0.011 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.01 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:   1 / 500 [  0%]  (Warmup)\nChain 4: Iteration:  50 / 500 [ 10%]  (Warmup)\nChain 4: Iteration: 100 / 500 [ 20%]  (Warmup)\nChain 4: Iteration: 150 / 500 [ 30%]  (Warmup)\nChain 4: Iteration: 200 / 500 [ 40%]  (Warmup)\nChain 4: Iteration: 250 / 500 [ 50%]  (Warmup)\nChain 4: Iteration: 251 / 500 [ 50%]  (Sampling)\nChain 4: Iteration: 300 / 500 [ 60%]  (Sampling)\nChain 4: Iteration: 350 / 500 [ 70%]  (Sampling)\nChain 4: Iteration: 400 / 500 [ 80%]  (Sampling)\nChain 4: Iteration: 450 / 500 [ 90%]  (Sampling)\nChain 4: Iteration: 500 / 500 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.006 seconds (Warm-up)\nChain 4:                0.004 seconds (Sampling)\nChain 4:                0.01 seconds (Total)\nChain 4: \n\n\nVisualize the distribution of values for each parameters and the chains convergence.\n\nplot(model)\n\n\n\n\nNow we will analyze and interpret the coefficients:\n\nsummary(model)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Sepal.Width ~ Sepal.Length + Petal.Length \n   Data: iris (Number of observations: 150) \n  Draws: 4 chains, each with iter = 500; warmup = 250; thin = 1;\n         total post-warmup draws = 1000\n\nRegression Coefficients:\n             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept        1.04      0.29     0.47     1.62 1.01      601      667\nSepal.Length     0.56      0.07     0.43     0.69 1.01      514      461\nPetal.Length    -0.33      0.03    -0.40    -0.28 1.01      529      437\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.33      0.02     0.29     0.36 1.00      626      620\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWe can see from the summary that our chains have converged sufficiently (rhat close to 1).\nThe length of the sepals seems to be a relevant predictor of the length of the petals for iris flowers, with a posterior mean regression coefficient of 1.78, 95% Credibility Interval [1.65, 1.90]. The length of the petals are also relevant predictors, with a posterior mean of -0.33, and a 95% credibility Interval of [-1.40, -0.28]. Since the intervals do not include zero, we can be fairly sure there is an effect.\nNow, we will analyze the posterior distribution:\n\npp_check(model, ndraws=100)\n\n\n\n\nAnalyzing the posterior, we see that the samples from the posterior predictive distribution (y_rep) seems to capture really well the observed data (y).\nThus, we learned that the sepal width is not independent on the length of the sepals and the length of the petals. We also learned the distributions of values for our model parameters that can significantly predict sepal width given the length of the sepals and petals.\nNext week we will see how to make our model better!\nWe can try other priors and then compare different models. We can also test interaction terms between the predictors, and check if all the predictor variables are really necessary."
  },
  {
    "objectID": "posts/new-post-with-code/index.html",
    "href": "posts/new-post-with-code/index.html",
    "title": "New Post With Code",
    "section": "",
    "text": "New post"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "What can you find here?",
    "section": "",
    "text": "Bayesian regression\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2024\n\n\nRenata B. Biazzi\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nApr 26, 2024\n\n\nRenata B Biazzi\n\n\n\n\n\n\nNo matching items"
  }
]